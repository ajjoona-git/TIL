# SSAFY AI 2 랩업 리포트

**작성자**: 박준아

**작성일**: 2025.10.30.


## Part 1. Wrap Up

### 1-1. 개요

#### 개발 주제

SSAFY AI 2 교육과정은 전통적인 머신러닝/딥러닝의 기초부터 최신 LLM(대규모 언어 모델) 기반의 AI 에이전트 개발까지, AI 애플리케이션 개발의 전 과정을 다뤘습니다. 이번 랩업 리포트는 0주차 선형 회귀 모델의 수동 구현부터 5주차 PEFT(LoRA)를 이용한 파라미터 효율적 튜닝에 이르기까지, 교육과정 전반의 핵심 실습 내용을 요약하고 개인적인 학습 성과를 회고하는 것을 목표로 합니다.

#### 개발 구현 내용 및 교육 내용과의 관련성

본 리포트는 교육과정 중 수행한 모든 실습 및 프로젝트의 학습 기록(TIL)을 기반으로 작성되었습니다.

  - **ML/DL Foundations**: NumPy 기반 선형/로지스틱 회귀, PyTorch 기반 MLP, CNN(ResNet-18), RNN/LSTM, Transformer 아키텍처
  - **Data Handling**: Pandas와 Matplotlib를 활용한 EDA(탐색적 데이터 분석) 및 시각화, Hugging Face `tokenizers`를 이용한 토큰화 및 임베딩
  - **Modern AI Models**: Vision Transformer(ViT)의 구조, CLIP을 활용한 이미지-텍스트 유사도 측정, LLM의 발전 과정(GPT, RLHF)
  - **AI Application Frameworks**: RAG(검색 증강 생성) 파이프라인 구축, ReAct 프레임워크 기반 AI 에이전트 개발, PEFT(LoRA)를 활용한 모델 경량화 튜닝

#### 개발 환경 및 협업 도구

  - **주요 라이브러리**: `PyTorch`, `Transformers`, `Diffusers`, `LangChain`, `Unsloth`
  - **데이터 처리**: `Pandas`, `NumPy`, `Matplotlib`, `Seaborn`
  - **개발 환경**: `Jupyter Notebook (Google Colab)`, `VS Code`
  - **협업 및 버전 관리**: `Git`, `GitHub`

#### 개발 구조 및 데이터셋 구조도

SSAFY AI 2 교육과정은 전통적인 ML에서 최신 LLM 에이전트로 이어지는 점진적 학습 구조를 따랐습니다.

```
[데이터 분석 (EDA)] -> [기초 ML 모델 (회귀)] -> [딥러닝 (MLP, CNN, RNN)] -> [트랜스포머 (Attention)] -> [파운데이션 모델 (LLM, ViT)] -> [모델 응용 (RAG, Agent)] -> [모델 최적화 (PEFT)]
```

### 1-2. 수행 목표 및 내용

#### 기획 및 목표

개인적인 목표는 'TIL (Today I Learned)' 저장소를 체계적으로 관리하는 것이었습니다. 매일의 학습 내용을 마크다운 파일로 정리하고, 관련 이미지와 프로젝트 로그를 함께 기록했습니다.

#### 주차별 실습 Road Map

  - **0주차 (ML Foundations)**: Python, NumPy를 사용해 선형 회귀와 로지스틱 회귀의 기본 원리를 구현. 경사 하강법(Gradient Descent)의 작동 방식을 이해.
  - **1주차 (DL & EDA)**: Pandas, Matplotlib를 활용한 EDA 기법 학습. PyTorch를 사용해 딥러닝의 기본 단위인 MLP(다층 퍼셉트론)를 구축하고 학습 사이클(순전파, 역전파, 옵티마이저)을 실습.
  - **2주차 (NLP Basics)**: Hugging Face `tokenizers`를 이용한 서브워드 토큰화(WordPiece)와 임베딩의 개념을 학습. RNN, LSTM의 한계(기울기 소실)와 이를 보완하는 어텐션, 트랜스포머 아키텍처를 배움.
  - **3주차 (Computer Vision)**: CNN의 핵심인 합성곱과 풀링을 이해하고, ResNet-18 모델로 Transfer Learning(전이 학습)을 수행. CLIP 모델로 생성된 이미지의 품질을 정량적으로 평가.
  - **4주차 (LLM & Agents)**: LLM의 한계(환각, 최신성 부족)를 극복하기 위한 RAG(검색 증강 생성) 파이프라인을 LangChain으로 구축. LLM이 외부 도구(Tool)를 사용하게 하는 ReAct 프레임워크 기반 AI 에이전트를 개발.
  - **5주차 (Efficient Tuning)**: LLM 전체를 재학습하는 것의 비효율성을 이해하고, PEFT 기법 중 하나인 QLoRA(양자화 + LoRA)를 Unsloth 라이브러리로 실습.

### 1-3. 개발 수행 결과

#### 탐색적 분석 및 전처리 (EDA)

Pandas의 `corr()`를 이용한 변수 간 상관관계 분석, Matplotlib/Seaborn을 활용한 히스토그램, 산점도, 페어 플롯 시각화를 통해 데이터의 분포와 이상치를 확인했습니다.

#### 프로젝트 핵심 구현 내용

SSAFY AI 2 과정의 핵심은 단순 모델 구현을 넘어, 최신 AI 모델을 '응용'하고 '최적화'하는 데 있었습니다.

1.  **RAG 기반 AI 에이전트 구축 (LangChain & ReAct)**
    LLM이 가진 환각(Hallucination)과 오래된 정보 문제를 해결하기 위해 RAG 파이프라인을 구축했습니다. LangChain Expression Language (LCEL)를 사용해 `Prompt | Model | Parser`의 흐름을 선언적으로 정의했습니다.

      - **Indexing**: PDF, TXT 등 비정형 문서를 로드하고, `RecursiveCharacterTextSplitter`로 의미 단위의 청크(Chunk)로 분할했습니다.
      - **Retrieval**: `UpstageEmbeddings` 모델로 청크를 벡터화하여 Chroma 벡터 스토어에 저장했습니다.
      - **Generation**: `create_retriever_tool`로 검색기(Retriever)를 에이전트가 사용 가능한 '도구(Tool)'로 만들었습니다.
      - **Agent**: `create_agent`를 통해 LLM이 ReAct 프레임워크(Reasoning + Acting)를 따르도록 설정했습니다. 에이전트는 "배송 정책" 질문이 들어오면 `Policy_Search` 도구(RAG)를, "주문 상태" 질문이 들어오면 `get_order_status_from_db` 도구(DB 조회)를 스스로 판단하여 호출했습니다.

2.  **PEFT를 활용한 모델 경량화 튜닝 (QLoRA)**
    수십억 개의 파라미터를 가진 LLM 전체를 파인튜닝하는 것의 막대한 비용 문제를 해결하기 위해, 파라미터 효율적 튜닝(PEFT) 기법을 학습했습니다.

      - **QLoRA**: 4비트 양자화(Quantization)로 모델의 무게를 줄이고, 원본 가중치는 고정한 채 소수의 어댑터(LoRA) 레이어만 학습하는 방식입니다.
      - **Unsloth 적용**: `FastLanguageModel`을 사용해 Phi-3 모델을 4비트로 로드하고, `load_adapter`를 통해 LoRA 어댑터를 장착했습니다.
      - **결과**: 영-한 번역 태스크 실습에서, 튜닝 전 모델은 "아침라는 종은 머리를 가지지 않으며..."처럼 오역했지만, LoRA 어댑터를 적용한 모델은 "무두족류는 머리가 없고..."로 정확하게 번역하며 파인튜닝의 효과를 명확히 확인했습니다.

#### 시연 결과 및 성능 지표

  - **RAG Agent**: 사용자의 질문 의도를 파악하여 적절한 도구(RAG 또는 DB 조회)를 선택하고, 외부 데이터를 근거로 환각 없이 정확한 답변을 생성했습니다.
  - **LoRA Fine-tuning**: 단 몇 분의 튜닝만으로 특정 도메인(번역)에 대한 모델의 성능이 극적으로 향상되는 것을 정성적으로 확인했습니다. 이는 적은 비용으로도 모델을 커스터마이징할 수 있음을 보여줍니다.

### 1-4. 자체 평가 의견

#### 잘한 점

  - **체계적인 학습 기록**: `ajjoona-git/til` 저장소를 운영하며, AI, 알고리즘, 웹, 프로젝트 등 분야별로 학습한 내용을 매일 기록했습니다. 이 기록들은 이번 리포트를 작성하는 데 핵심 자료가 되었을 뿐만 아니라, 지식이 파편화되는 것을 막고 큰 그림을 볼 수 있게 도왔습니다.
  - **최신 기술 스택에 대한 적극성**: MLP, CNN 같은 전통적 딥러닝을 넘어, RAG, Multi-Agent, LoRA 등 현재 AI 산업에서 가장 활발히 논의되는 주제들을 깊이 있게 다뤘습니다.

#### 시도했으나 잘 되지 않았던 것

  - **AI 모델 평가의 어려움**: `LLM-as-judge` 실습에서 AI가 평가할 때조차 위치 편향이나 길이 편향이 존재한다는 것을 배웠습니다. 이는 모델의 성능을 객관적으로 측정하는 것이 얼마나 어려운지, 그리고 단순히 '정확도' 외에 다양한 평가 지표가 왜 필요한지 깨닫게 했습니다.

#### 프로토타입 개발을 통해 배운 점

이번 SSAFY AI 2 과정은 'AI 모델을 만드는 개발자'에서 'AI 모델을 활용해 서비스를 만드는 개발자'로 시야를 넓히는 계기가 되었습니다. 특히 RAG와 PEFT 실습은, 모든 것을 처음부터 학습시키는 대신 이미 잘 훈련된 파운데이션 모델을 '레고'처럼 조립하고 '튜닝'하여 특정 목적에 맞게 최적화하는 현대 AI 개발의 패러다임을 명확히 이해하게 했습니다.

MLP의 `loss.backward()`부터 ReAct 에이전트의 `agent.invoke()`까지, AI 기술이 어떻게 추상화되고 발전해왔는지 그 흐름을 꿰뚫는 경험이었습니다. 앞으로는 이 '도구'들을 조합하여 실제 사용자가 가치를 느낄 수 있는 서비스를 구현하는 데 집중할 것입니다.



## Part 2: AI 챌린지

VQA(Visual Question Answering) 챌린지 과제 해결을 위해, 베이스라인 모델에서부터 최종 제출 모델에 이르기까지 팀이 수행한 기술적 접근과 실험 과정을 기록하고 분석합니다.

### 2.1. VQA 모델 아키텍처 선정 및 검증

- 초기 모델 풀(Pool) 비교: VQA 태스크  수행을 위해 Google paligemma, Alibaba Qwen, LLaVA, BLIP 등 현존하는 SOTA(State-of-the-Art) VQA 모델들의 초기 성능을 검토했습니다.

- Qwen 모델의 초기 우위 확보: 벤치마킹 결과, Qwen 모델이 "가장 높은 성능" 을 보였습니다. 제공된 베이스라인 코드의 초기 점수(0.86831)  역시 Qwen을 기반으로 하고 있어, Qwen을 중심으로 고도화 전략을 수립하는 것이 가장 합리적인 접근으로 판단되었습니다.

- Qwen 포트폴리오 분석 및 최종 모델 채택: Qwen 모델군 내에서도 세부적인 비교가 이루어졌습니다.
  - `Qwen3-VL-8B`와 같은 고성능 모델은 성능이 탁월하나, "한정된 시간 자원" 내에 파인튜닝을 완료하기 어렵다고 판단되었습니다.
  - `Qwen2.5-Omni` 등은 오디오/비디오 입력을 지원하여 본 태스크의 범위를 넘어선 비효율적인 아키텍처로 간주되었습니다.
  - 최종적으로 "a~d 선택형 QA에 적합"하고 "문서 및 일상 이미지 처리" 능력이 우수하며, 주어진 자원 제약 내에서 효과적인 튜닝이 가능한 `Qwen2.5-VL-7B-Instruct`가 핵심 모델로 채택되었습니다.

- 모델 선정 과정은 '절대적 최고 성능'이 아닌 '한정된 자원 내 실현 가능한 최고 성능'을 추구하는 실용주의적(pragmatic) 접근을 기반으로 했습니다.

### 2.2. 데이터 파이프라인 구축 및 정제

프로젝트는 0.94라는 성능 한계에 직면했으며, 이는 모델 중심(Model-Centric) 접근에서 데이터 중심(Data-Centric) 접근으로의 전략적 전환을 만든 변곡점이었습니다.

#### **이미지 데이터 처리:**

초기에는 이미지 크기 분포를 확인 하고 224px 등 특정 크기로 규격화하는 실험이 진행되었습니다.

고도화 과정에서는 단순한 이미지 리사이징이 아닌 'Letterbox(정사각 패딩)' 기법을 도입했습니다. 이는 원본 이미지의 종횡비를 유지하면서 패딩을 추가하여, 정보 손실을 최소화하고 모델에 일관된 입력 형태를 제공하기 위함입니다.

최적 해상도를 탐색하기 위해 224px 에서 448px, 896px 까지 실험이 이루어졌으며, 최종적으로 최고 성능 모델(0.93724)은 448px 해상도를 채택했습니다.

#### **텍스트 데이터 처리 (기각된 가설):**

- 가설: Qwen이 다국어 처리가 가능하지만 "영어에 최적화" 되어 있다는 점에 착안, 텍스트(질문) 데이터를 영어로 번역한 후 학습 및 추론하는 전략을 수립했습니다.

- 검증: 실험 로그("89351 | 번역 후 덮어쓰기")는 동일 조건의 미번역 실험("93158 | x") 대비 0.89351점 대 0.93158점으로 현저한 성능 하락을 보였습니다.

- 결론: "유의미한 결과를 확인하지 못했다" 는 결론 하에 텍스트 번역 파이프라인은 즉시 폐기되었습니다. 이는 Qwen의 한국어 처리 능력이 번역 오버헤드를 감수할 만큼 영어 대비 열등하지 않음을 시사합니다.

#### **노이즈 데이터 식별 및 정제:**

- 문제 정의: 거듭된 하이퍼파라미터 튜닝에도 불구하고 0.94 점수를 넘지 못하는 '성능 정체기(plateau)'에 봉착했습니다.

- 원인 분석 (Data-Centric): 원인을 모델 튜닝이 아닌 데이터 자체의 품질 문제로 전환, 훈련 데이터를 수동 검수(Manual Inspection)하는 데이터 중심 접근을 채택했습니다.

- 노이즈 식별: 검수 결과, "사람이 확인해도 정답을 도출하기 어려운 문제", "전문적인 지식을 요하거나", "구체적인 음식 종류"를 식별해야 하는 문제, "이미지의 텍스트를 분석해야하는" 문제 등 VQA 모델의 일반적인 시각적 추론 범위를 벗어나는 데이터가 '노이즈'로 규정되었습니다.

- 조치: 해당 노이즈 데이터는 모델 학습에 방해가 된다고 판단하여 "삭제하는 조치" 를 취했습니다.

0.94의 1차 성능 한계를 돌파한 것은 모델이나 하이퍼파라미터가 아닌 '데이터 품질' 개선이었습니다. 이 '노이즈 데이터 제거'라는 데이터 중심(Data-Centric) 접근은 이후 2.3절과 2.4절에서 수행된 모든 파라미터 튜닝이 유의미한 결과를 내기 위한 핵심적인 전제 조건(prerequisite)이 되었습니다.

### 2.3. 하이퍼파라미터 튜닝 및 실험적 탐색

데이터 정제 이후, 모델의 성능을 미세 조정하기 위한 실험적 탐색이 의 로그를 기반으로 체계적으로 진행되었습니다.

#### **max_new_token의 발견:**

챌린지 요건 은 'a/b/c/d' 중 하나의 정답을 출력하는 단일 글자(Single Character) 태스크입니다.

논리적으로는 `max_new_token=1` 이 최적일 것으로 예상했으나, 실제 실험 결과 `max_new_token=2` 가 월등히 높은 성능을 기록했습니다. `max_new_token=4` 로 늘렸을 때는 다시 성능이 하락, '2'가 최적값임을 확인했습니다.

`max_new_token=2`의 우위는, VQA 모델이 본질적으로 'open-ended'  답변을 생성하도록 설계되었기 때문에, 'a'라는 단일 토큰이 아닌 "a." 또는 " a" (공백 포함) 등 1개 이상의 토큰을 생성하려는 경향이 있음을 시사합니다. 이는 향후 과제에 "데이터 파싱 부분 고도화"의 필요성을 뒷받침합니다. `max_new_token=2` 설정은 이 근본적인 불일치 문제를 임시적으로 해결하는 '실용적인 해법(pragmatic hack)'이었던 셈입니다.

#### **학습률(Learning Rate) 스케줄링:**

초기 `1.00E−04` 에서 시작하여 `8.00E−05` , `6.00E−05`  등으로 점진적인 하향 탐색이 이루어졌습니다.

학습률 `6e−5`는 최고 성능 모델(0.93724)에서 스케줄러와 함께(`6.9e−5` ∼ `1.6e−7`) 사용되며 최적값임이 입증되었습니다.

#### **Epoch 및 데이터셋:** 초기 3,000개 샘플, 1-epoch 실험에서, 데이터 정제 및 증강 을 거쳐 7,774개 샘플, 3-epoch 훈련 으로 확대되며 모델의 학습 완성도와 성능 향상을 견인했습니다.

### 2.4. LoRA/QLoRA 기반 파인튜닝 고도화

"한정된 자원"  내에서 7B급의 거대 모델 을 튜닝하기 위해 QLoRA  기법이 도입되었으며, LoRA 하이퍼파라미터에 대한 집중적인 실험이 이루어졌습니다.

#### **LoRA 파라미터(r, lora alpha) 탐색:**

`r` (LoRA 행렬의 랭크, 모델의 표현력)과 `lora_alpha` (LoRA 보정값의 가중치) 에 대해 다양한 조합이 시도되었습니다.

실험 로그(`r=32`, `alpha=32`) 와 (`r=32`, `alpha=64`)  등을 비교했습니다.

최종 최고 성능 모델(0.93724)은 (`r=32`, `alpha=32`) 조합을 사용 , 이 조합이 32/64 조합보다 더 안정적이거나 우수한 성능을 보였음을 시사합니다.

#### **LoRA 튜닝 최적화 (실시간 기술 습득):**

"r 값 올리면 다른 것도 올려줘야 하는 게...": r과 `lora_alpha`, `BASE_LR` 간의 상호 연관성을 인지하고 파라미터를 함께 수정하기 시작했습니다.

LoRA 파라미터가 옵티마이저에 의해 불필요하게 감쇠(decay)되는 것을 막는, SOTA(State-of-the-Art)의 핵심 튜닝 기법입니다. 이 기법의 적용은 튜닝 효율을 극대화하는 데 기여했습니다.

- 팀은 QLoRA 를 통해 베이스 모델을 양자화하여 VRAM 사용량을 줄였습니다. 그리고 이 확보된 VRAM을 (1) 더 높은 해상도의 이미지(448px) 처리와 (2) 더 풍부한 표현력의 LoRA 레이어(r=32) 학습에 전략적으로 재투자했습니다. `WeightDecay=0.0` 설정은 이 재투자의 효율을 극대화했습니다. 이는 VRAM, 모델 표현력, 데이터 품질 간의 복잡한 트레이드오프를 성공적으로 관리했음을 보여줍니다.

- **훈련 안정화:** 얼리 스탑핑(Early Stopping)을 도입하고 `best_step`을 저장 함으로써, 3-epoch  훈련 시 발생할 수 있는 과적합을 방지하고 최적의 체크포인트를 확보하는 안정화 전략을 사용했습니다.

### 2.5. 최종 제출 전략: 프롬프트 엔지니어링 및 앙상블

단일 모델의 성능을 넘어서기 위해 후처리 및 앙상블 전략이 사용되었습니다.

- 객관식 프롬프트 엔지니어링: VQA 모델의 출력이 'open-ended' 가 아닌, 챌린지 요구사항 인 "a/b/c/d 중 하나의 답을 출력" 하도록 강제하는 프롬프트를 설계했습니다.

#### **앙상블 (Voting) 기법:**

voting 방법을 도입했으며, 초기에는 0.90 이상인 결과, 상위 3개의 결과 등 여러 가지 조합을 시도했습니다.

최종적으로 성능이 가장 높은 상위 3개 모델을 선정했습니다.

단순 다수결(Hard Voting)이 아닌, Softmax 출력값을 활용하여, 확률 평균값이 가장 높은 클래스를 최종 정답으로 채택하는 Soft Voting 방식을 사용했습니다. 이는 개별 모델의 확신도(confidence)까지 반영하여 견고성을 높이는 고급 앙상블 기법입니다.

앙상블을 통해 최종 스코어 0.94341 을 달성했습니다. 이는 단일 모델 최고점인 0.93724 를 상회하는 수치로, 앙상블 전략이 성공적으로 작동했음(선정된 모델들이 서로의 오류를 보완했음)을 입증합니다.

### 2.6. 최종 성과 및 핵심 모델 분석

- 최종 성과: 개별 모델의 Softmax 확률 값을 평균 내어 최적의 답을 도출하는 Soft Voting 앙상블 을 통해 최종 스코어 0.94341을 달성했습니다.

- 단일 모델 최고 성과: 앙상블의 기반이 된 단일 모델 중 최고 성능은 0.93724 를 기록했습니다.


**[표 1: 단일 모델 최고 성능(0.93724) 파라미터]**

| 구분 | 파라미터 | 값 | 분석 및 근거 (Snippets) |
|------|------|------|------|
| **모델** | 베이스 모델 | Qwen/Qwen2.5-VL-7B-Instruct | Qwen2.5가 핵심 |
| | Loss (Train/Val) | 5.3675 / 5.3314 | 높은 Loss에도 높은 점수 |
| **데이터** | Image Size | 448 (processor min/max pixels 고정) | 224px 대비 고해상도 전략 |
| | Image Augmentation | letterbox (정사각 패딩) | 원본 비율 유지, 정보 손실 최소화 |
| | Text Translation | X (미적용) | 번역 시 성능 하락 |
| **튜닝 (LoRA)** | $r$ (rank) | 32 ||
| | $rora\_alpha$ | 32 | 32/64 조합 대비 32/32 조합이 우위 |
| **튜닝 (Training)** | $max\_new\_token$ | 2 | 1-char 문제임에도 2가 최적 |
| | epoch | 3 | 1-epoch 대비 충분한 학습 |
| | batchsize | 4 |  |
| | 학습률 (LR) | $6e-5$ (스케줄러: $6.9e-5 \sim 1.6e-7$) | $1e-4$ 대비 하향 조정된 최적값 |
| | temperature | 0 | 생성의 무작위성을 배제, 결정론적(deterministic) 출력 유도 |


### 2.7. 핵심 성공 요인

#### 1. 0.94의 벽을 넘게 한 '데이터 중심(Data-Centric)' 접근:

프로젝트의 가장 큰 변곡점은 0.94 성능 정체기 였습니다.

이때 원인을 '모델'이 아닌 '데이터'로 규정하고, 수동 검수를 통해 "사람이 풀어도 어려운" 노이즈 를 식별하고 제거한 것이 제1의 성공 요인입니다. 이는 후속 튜닝의 효과를 극대화하는 기반이 되었습니다.

#### 2. QLoRA 기반 'VRAM-성능' 트레이드오프 전략:

단순히 QLoRA 를 사용한 것이 아니라, QLoRA로 확보한 VRAM을 (1) 고해상도 이미지(448px) 처리 와 (2) 고차원 LoRA(r=32) 에 '재투자'한 전략이 주효했습니다.

#### 3. SOTA 튜닝 기법의 신속한 학습 및 적용:

LoRA 파라미터에 `weightdecay=0.0` 을 적용한 것은 교과서적인 튜닝을 넘어, SOTA 커뮤니티의 프랙티스를 실시간으로 학습하고 적용했음을 보여줍니다. 이는 `r=32`의 표현력을 감쇠 없이 훈련시키는 데 결정적이었을 것입니다.

#### 4. max_new_token=2라는 비직관적 파라미터의 실험적 발견:

태스크 정의('a')에 매몰되지 않고, `max_new_token=1`(92489점)과 `max_new_token=2`(93672점)를 비교 실험하여 비직관적인 최적값 '2'를 발굴 해낸 것이 큰 점수 향상(약 1200점)을 가져왔습니다.

### 2.8. 비효율적 접근 및 기각된 가설
- 가설 기각 (텍스트 번역): 의 "영어 최적화" 가설은 의 "준아 | 89351" 실험(89k)과 "준아 | 93158" 실험(93k)의 직접 비교를 통해 '명백히 실패'로 검증되었습니다. 이는 리소스를 조기에 회수(pivot)할 수 있게 한 가치 있는 실패였습니다.

- 평가 지표의 함정 (Loss vs. Accuracy):

  - '점수는 높은데 로스가 커졌다': 이는 챌린지의 평가지표가 'Accuracy'(정확도) 인 반면, 모델의 훈련 목표는 'Cross-Entropy Loss'라는 괴리에서 발생합니다. '노이즈 데이터'(예: 전문 지식)는 모델이 사실상 맞출 수 없는 샘플이었을 것이며, 이는 'Loss'를 무한히 높이는 주범이 됩니다.

  - 따라서 'Loss'가 높다는 것은 모델이 '노이즈'를 학습하길 포기했다는 신호일 수 있으며, 'Accuracy'가 높다는 것은 '노이즈가 아닌' 대부분의 샘플을 잘 맞추고 있다는 의미입니다. 팀이 'Loss'에 매몰되지 않고 'Accuracy'를 기준으로 데이터 정제 를 수행한 것은 올바른 전략적 판단이었습니다.


## Part 3: 개인 회고

## 3.1. 프로젝트 회고 및 교훈

#### 모델 상태의 정확한 진단이 튜닝 방향을 결정한다:

  - dropout: 과적합일 때 사용할만한데 지금 우리 모델은 대부분 언더피팅
  - '언더피팅'이라는 이 정확한 진단은 (1) 과적합 방지 기법인 Dropout을 적용하지 않고, (2) epoch 수를 1에서 3으로 늘리며 , (3) LoRA weightdecay를 0 으로 설정하는 등, 모델의 표현력과 훈련 강도를 높이는 일관된 튜닝 전략의 결정적 근거가 되었습니다.

#### 실험 로그는 가장 중요한 자산이다:

  - 상세한 실험 로그(담당자, 파라미터, 점수, 그리고 "٩(இ ⌓ இ๑)۶"와 같은 주석 포함)
  - 이는 팀원 간의 비동기적 협업을 가능하게 하고, "정현 | 93518" 실험의 weightdecay 발견처럼 한 사람의 기술적 발견이 팀 전체의 자산으로 즉시 전파되게 하는 핵심 인프라였습니다.

### 3.2. 향후 연구 및 미시도 아이디어

#### 1. 데이터 파싱 고도화 (최우선 과제):

max_new_token=2 라는 현재의 '임시방편(hack)'을 넘어서야 합니다.

'open-ended' VQA의 원시 출력(raw data)을 안정적으로 'a/b/c/d'로 매핑하는 후처리 파싱(Data Parsing) 로직 을 정교하게 개발해야 합니다. 이는 모델의 견고성(robustness)을 높이는 데 필수적입니다.

#### 2. 모델 분기(Branching) 아키텍처 도입:

시간 제한으로 인해 시도하지 못한 아이디어로, 단일 VQA 모델이 모든 것을 처리하게 하는 대신, 질문의 '유형'을 먼저 분류해야 합니다.

"이미지의 텍스트를 분석" 해야 하는 문제는 OCR 모델로, "객체 탐지" 가 필요한 문제는 Detection 모델로 작업을 분기(branch) 처리하는 'Mixture-of-Experts' 또는 'RAG'와 유사한 접근이 필요합니다.

#### 3. 체계적인 오답 분석 (Error Analysis):

(정현 아이디어)에서 제안된 "answer열과 predicte 열 비교해서 오답 행 추출해내고, 해당 이미지와 문제 사람이 직접 탐색해보기"는 0.94의 벽을 넘게 한 '데이터 정제' 를 0.943 이상으로 끌어올릴 수 있는 '제2의 데이터 중심' 접근법입니다. 이는 다음 튜닝 사이클의 시작점이 되어야 합니다.

#### 4. K-Fold 교차 검증:

현재의 단일 Val/Test 스플릿이 아닌, K-Fold 교차 검증 을 도입하여 모델의 일반화 성능을 더욱 견고하게 측정하고 데이터셋 편향의 위험을 줄여야 합니다.

### 3.3. 자체 평가 및 시사점

- **종합 평가:** 본 학습 내용 정리를 통해 머신러닝의 이론적 기초부터 최신 LLM 및 AI 에이전트 기술, 그리고 이를 실제 개발에 적용하는 방법에 이르기까지 AI 기술 전반에 대한 폭넓고 체계적인 시야를 확보했습니다.

- **잘한 점:** 각 기술 요소(예: CNN, Transformer, RAG)의 핵심 아이디어와 발전 과정을 논리적으로 연결하여 기술의 흐름을 효과적으로 요약했습니다.

- **아쉬운 점:** 제공된 `AI/` 폴더의 각 파일이 개별 주제에 대한 학습 노트(TIL) 형식이므로, 이를 엮어 하나의 '프로토타입 개발' 사례 보고서로 구성하기보다는 학습 내용을 종합하는 리포트 형식으로 작성되었습니다.

- **시사점:** AI 코딩 어시스턴트와 에이전트 기술의 발전은 개발자의 역할을 '모든 것을 직접 구현하는 사람'에서 'AI에게 정확한 목표와 제약조건을 지시하고 결과를 검수하는 관리자'로 변화시키고 있음을 확인했습니다. 이에 따라 명확한 시스템 설계와 요구사항 정의(PRD) 능력이 미래 개발자에게 더욱 중요해질 것입니다.