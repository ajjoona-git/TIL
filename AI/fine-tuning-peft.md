## 적응 학습

### 미세 조정 (Fine-tuning)

- 추가학습을 통해 이미 학습된 모델을 조금만 튜닝하는 것
- 특정 작업에 특화된 모델을 개발할 수 있다.
- 사전 학습된 모델에 프롬프팅을 통한 작업을 했을 때보다 더 좋은 퀄리티의 결과물을 생성
- 프롬프트에 넣는 예제보다 훨씬 더 많은 예제를 통해 학습 가능
- 프롬프트의 길이가 줄어들면서 토큰 개수 절약
- 응답하는 데 걸리는 시간(latency) 단축

### 학습률 (Learning Rate)

- 손실함수가 큰 값일 때 미세하게 조정하기 어려우므로 뉴럴넷 모델에 작은 비율로 반영한다.
- 모델과 데이터마다 적절한 learning rate가 다르기 때문에 실험을 통해 구해야 함
    - 좋은 시작점에서부터 시작하기 때문에 작은 learning rate부터 보수적으로 시작해야 함

### PEFT (Parameter-Efficient Fine-Tuning)

- 프롬프트 디자인
    - 언어모델에서 주로 활용
    - 모델이 원하는 레벨의 결과를 출력할 수 있도록 입력 텍스트를 변형하는 방법
    - 추가 학습 없이 사전학습된 모델의 예측 성능을 끌어올릴 수 있음
    - 프롬프트를 사람이 직접 설계해야 함
    - 성능 향상이 제한적
- 프롬프트 튜닝: 학습 가능한 프롬프트
    - 가상 토큰 (virtual token)을 입력에 추가
    - 역전파를 통해 오직 가상 토큰에 대한 임베딩만 학습하고 나머지 모델은 고정

![프롬프트 튜닝](../images/fine-tuning-peft_1.png)

프롬프트 튜닝

- Adaptor 모듈 추가 학습
    - Activation을 변경하기 위해 작은 모듈을 추가하여 학습하는 기법
    
    ![Adaptor 모듈](../images/fine-tuning-peft_2.png)
    
    Adaptor 모듈
    

## 학습 데이터 활용

### Knowledge Distillation (Teacher-Student 학습)

- 지금까지 배운 미세조정과 같은 전이학습은 사전학습된 모델과 새로 학습할 모델의 구조가 동일한 구조를 가정하고 있다.
- 지식증류: 높은 성능의 무거운 모델(선생님)을 모방하도록 가벼운 모델(학생)을 학습하는 방법
- 크기가 작은 모델만으로 충분히 학습하기 어려운 데이터 특징을 학습하기 위해 비교적 무겁고 성능이 높은 모델의 도움을 받는 기법
- 선생 모델이 예측한 soft-label 값과 학생 모델의 예측 값이 가까워지도록 학습 유도

![지식 증류](../images/fine-tuning-peft_3.png)

### InstructPix2Pix

- 파운데이션 모델들을 툴로 활용하는 방법
- 명령(instruction)에 따라 이미지 편집을 수행하는 모델
- 입출력 이미지 상세 설명 없이 명령만으로 편집 수행
    - {이미지 편집에 대한 지시사항, 편집 전 이미지, 편집 후 이미지} 형식의 학습 데이터셋 생성
    - 입력 데이터-정답 쌍 필요
